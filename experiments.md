# 도메인 분류 실험 기록

## 실험 1~17 (요약)

- **배경**: 초기 21개 도메인 체계의 모호함으로 인해 정확도가 40%대에서 정체.
- **전환점**: 실험 16, 17을 통해 "구체적인 행동 의도(Micro-Intent)"가 LLM에게 훨씬 효과적임을 확인.
- **조치**: 전체 데이터셋(695개)의 Ground Truth를 42개 Micro-Intent로 전면 재정의 (`input/input_new_gt.xlsx`).

---

## 실험 18: 새로운 Ground Truth 검증 (50개 샘플)

- **날짜**: 2025-12-21
- **결과**: **76.00% (38/50)**
- **핵심 발견**:
  1.  **정확도 급상승**: 기존 20~40%대에 머물던 정확도가 단숨에 76%로 도약. LLM은 "추상적 범주(예: 계약정보)"보다 "구체적 행위(예: 주소변경)"를 훨씬 잘 이해함.
  2.  **RAG 필터로서의 한계**: 76%는 분류기로서는 우수하나, 검색 필터(Hard Filter)로 쓰기에는 **24%의 오답률(검색 실패)**이 너무 높음. 필터링이 틀리면 정답 문서를 아예 찾을 수 없으므로, **98% 이상의 재현율(Recall)**이 필수적임.
  3.  **오답 패턴**: 미묘한 의도 차이(예: 부활 보험료 납입 -> `보험료 납부` vs `부활 신청`)에서 주로 발생. 이는 단일 정답만 강요할 때 발생하는 필연적 한계.

---

## 실험 19: Top-3 다중 의도 추론 (Recall 극대화)

- **날짜**: 2025-12-21
- **목표**: 단일 분류의 불안정성을 보완하여, 정답 도메인이 검색 대상에 포함될 확률(Recall)을 **98% 이상**으로 끌어올림.
- **가설**: "가장 적절한 것 1개"를 고르라고 하면 틀릴 수 있지만, "가능성 높은 3개"를 고르라고 하면 그 안에 정답이 포함될 확률은 매우 높을 것이다.
- **주요반영사항**:
  1. **프롬프트 강화**:
     - "가장 정확한 하나를 선택하세요" → "**가능성이 높은 순서대로 최대 3개를 나열하세요**"
     - **절대 금지 사항**: "기타", "없음", "알 수 없음" 같은 답변 금지
     - **Few-shot 예시 3개** 추가 (실제 문제 케이스 포함)
     - **강제 선택 지시**: 완전히 일치하지 않더라도 가장 가까운 의도 선택
  2. **응답 형식 변경**: `도메인1:`, `도메인2:`, `도메인3:` 형식으로 순위별 도메인 반환
  3. **평가 기준 변경 (Hit@K)**: LLM이 제시한 Top-K 의도 중 하나라도 Ground Truth와 일치하면 **성공(Pass)**으로 판정
  4. **Fuzzy Matching 완화**:
     - Threshold: 0.6 → **0.4** (유사도 40% 이상이면 매칭)
     - **강제 매칭**: 0.4 미달 시에도 best_match 사용 (미분류 최소화)
  5. **결과 저장 강화**: `classified_domains` (Top-K 전체), `hit_rank` (Hit된 순위) 필드 추가
- **구현 위치**:
  - `src/llm_classifier.py`: `_build_prompt()`, `_parse_response()`, `classify()` 메서드 수정
  - `main.py`: `process_single_item()` - Hit@K 평가 로직 구현, `save_json_result()` - 다중 도메인 저장
- **RAG 적용 시나리오**: 실제 검색 시, 예측된 Top-K 도메인의 문서를 모두 검색 범위(OR 조건)에 포함하여 Recall 극대화
- **기대 효과**: 검색 범위를 약간 넓히는(Precision 희생) 대신, 정답을 놓치지 않는(Recall 확보) 안전한 검색 시스템 구축
- **결과 (100개 샘플)**:
  - **전체 정확도**: 68.00% (68/100) - 실험18 대비 **-8%p 하락**
  - **GT 미분류 제외 정확도**: **82.93% (68/82)** - GT가 설정된 케이스만 평가
  - **Hit@K 분포**:
    - Hit@1: 67건 (67%)
    - Hit@2: 1건 (1%)
    - Hit@3: 0건 (0%)
  - **Top-K 반환 현황**: 88건 1개만, 12건 2개, 0건 3개 - **Top-3 전략 실패**
  - **강제 매칭 문제**: 22건이 "기타" → "심사 기준/결과" (유사도 0.25)로 강제 매칭
- **문제점**:
  1. **LLM이 Top-3 지시 무시**: 대부분 1개만 반환하여 Recall 향상 효과 없음
  2. **강제 매칭의 부작용**: 유사도 0.25로 "심사 기준/결과"에 강제 매칭 (22건 오분류)
  3. **프롬프트 개선 한계**: Few-shot 예시와 강제 지시에도 LLM이 "기타" 반환
- **핵심 발견**:
  - **Top-K 전략은 LLM 협조 없이는 효과 없음**: LLM이 여러 개를 반환해야 하는데 1개만 반환
  - **강제 매칭은 역효과**: Threshold 없이 강제 매칭 시 무의미한 도메인으로 분류
  - **실험18 (단일 분류, 76%) > 실험19 (Top-3, 68%)**: 오히려 성능 하락
- **개선 방향 제안**:
  1. **강제 매칭 제거**: 유사도 0.4 미달 시 강제 매칭하지 않고 "미분류" 처리
  2. **Threshold 0.3 적용**: 최소한의 유사도 기준 유지 (0.3 이상만 매칭)
  3. **실험18로 복귀 고려**: Top-3 전략보다 단일 분류가 더 효과적
  4. **LLM 변경 고려**: 현재 LLM(Databricks GPT-OSS 20B)이 지시 따르기 취약
     - 대안: GPT-4, Claude 등 instruction-following 성능이 높은 모델
     - 또는 프롬프트 엔지니어링 대신 구조화된 출력 강제 (JSON mode 등)

---

## 실험 20: 강제 매칭 제거 + Threshold 0.3 적용 (전수 테스트)

- **날짜**: 2025-12-21
- **목표**: 실험 19의 강제 매칭 부작용을 해결하고, 전체 데이터셋(695개)에 대한 전수 테스트 수행
- **가설**:
  1. 강제 매칭을 제거하면 무의미한 분류(0.25 유사도 매칭)가 사라져 정확도 향상
  2. Threshold 0.3 적용으로 합리적인 유사도 기준 유지하면서 미분류 비율 감소
  3. Top-3 전략은 유지하되, LLM이 1개만 반환하더라도 정확도 개선 기대
- **주요반영사항**:
  1. **강제 매칭 로직 제거**:
     - 기존: 유사도 0.4 미달 시에도 best_match 사용 → 0.25 유사도로 "심사 기준/결과"에 강제 매칭
     - 변경: 유사도 Threshold(0.3) 미달 시 `미분류-{원본}` 처리
  2. **Threshold 조정**:
     - 0.4 → **0.3**으로 하향 조정
     - 최소한의 유사도 기준 유지하면서 매칭 범위 확대
  3. **Top-3 전략 유지**:
     - LLM이 Top-3를 반환하지 않는 문제는 있지만, 프롬프트와 평가 로직은 유지
     - 향후 LLM 변경 시 효과를 볼 수 있도록 구조 보존
  4. **전수 테스트**:
     - 샘플링 없이 전체 데이터셋 695개 테스트
     - 실제 운영 환경 적용 전 전체 성능 검증
- **구현 위치**:
  - `src/llm_classifier.py`: `classify()` 메서드 - 강제 매칭 로직 제거 (lines 153-157), Threshold 0.3 적용 (line 146)
- **기대 효과**:
  - 실험 19 대비 정확도 향상 (강제 매칭으로 인한 오분류 22건 제거)
  - 전수 테스트를 통한 전체 시스템 성능 파악
  - RAG 필터로서의 실제 활용 가능성 평가
- **결과 (695개 전수 테스트)**:
  - **전체 정확도**: **80.14% (557/695)** - 실험19 대비 **+12.14%p 대폭 향상**
  - **GT 미분류 제외 정확도**: **83.36% (486/583)** - GT가 설정된 케이스만 평가
  - **GT 미분류 케이스**: 112건 (16.12%) - 아직 GT가 설정되지 않은 데이터
  - **Hit@K 분포**:
    - Hit@1: 544건 (78.27%)
    - Hit@2: 13건 (1.87%)
    - Hit@3: 0건 (0%)
    - Miss: 138건 (19.86%)
  - **Top-K 반환 현황**: 1개 586건 (84.3%), 2개 100건 (14.4%), 3개 9건 (1.3%)
  - **미분류 케이스**: 153건 (22.01%) - 모두 LLM이 '기타' 반환
- **핵심 발견**:
  1. **강제 매칭 제거의 효과 입증**: 실험 19 대비 정확도 12.14%p 대폭 향상
     - 실험 19: 유사도 0.25로 "심사 기준/결과"에 강제 매칭 → 오분류 발생
     - 실험 20: 유사도 0.3 미달 시 미분류 처리 → 틀린 답을 강요하지 않음
  2. **Threshold 0.3의 효과**: 0.4 → 0.3으로 낮춰 매칭 범위 확대, 합리적인 매칭 증가
  3. **전수 테스트 성공**: 695개 전체 데이터에 대해 80% 이상의 안정적인 성능 달성
  4. **Top-3 전략은 여전히 미작동**: LLM이 여전히 대부분 1개만 반환 (84.3%)
     - 하지만 2개, 3개 반환 케이스가 증가 (실험19: 12%, 0% → 실험20: 14.4%, 1.3%)
     - Hit@2가 13건 발생 → Top-K 전략이 일부 케이스에서 효과 시작
  5. **미분류 케이스 증가**: 153건 (22.01%)
     - 모두 LLM이 프롬프트를 무시하고 '기타' 반환
     - 하지만 강제로 틀린 답을 매칭하는 것보다 훨씬 나은 결과
  6. **RAG 필터로서의 가능성**: 83.36% 정확도는 검색 필터로 사용하기에 충분히 높은 수준
     - 틀린 답을 강요하지 않아 Precision 향상
     - Top-K 전략으로 Recall도 소폭 개선 (Hit@2 13건)
- **남은 과제**:
  1. **미분류 케이스 153건 처리**: LLM이 '기타'를 반환하는 케이스 개선 필요
     - 프롬프트 개선 또는 LLM 모델 변경 고려
  2. **Top-3 전략 활성화**: LLM이 Top-3를 반환하도록 유도
     - 현재 모델(Databricks GPT-OSS 20B)의 instruction-following 한계
     - GPT-4, Claude 등 고성능 모델로 변경 또는 JSON mode 강제
  3. **Hit@2, Hit@3 활용**: 현재 Hit@2 13건, Hit@3 0건
     - Top-K 전략이 활성화되면 Recall 대폭 향상 가능
